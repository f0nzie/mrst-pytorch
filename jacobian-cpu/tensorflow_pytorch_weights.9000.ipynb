{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/msfz751/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/msfz751/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/msfz751/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/msfz751/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/msfz751/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/msfz751/anaconda3/envs/machine-learning/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500  # Input size\n",
    "H = 100  # Hidden layer size\n",
    "O = 10   # Output size\n",
    "\n",
    "w1 = np.random.randn(N, H)\n",
    "b1 = np.random.randn(H)\n",
    "\n",
    "w2 = np.random.randn(H, O)\n",
    "b2 = np.random.randn(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Numpy implementation\n",
    "\"\"\"\n",
    "\n",
    "def ffpass_np(x):\n",
    "    a1 = np.dot(x, w1) + b1   # affine\n",
    "    r = np.maximum(0, a1)     # ReLU\n",
    "    a2 = np.dot(r, w2) + b2   # affine\n",
    "    \n",
    "    exps = np.exp(a2 - np.max(a2))  # softmax\n",
    "    out = exps / exps.sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.49472312e-238, 3.22041051e-089, 1.93599195e-182, 1.40464542e-105,\n       2.90736603e-025, 5.86590900e-086, 2.10487618e-051, 1.00000000e+000,\n       5.19955060e-095, 4.19231708e-034])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.random((N,))\n",
    "out_np    = ffpass_np(x0)\n",
    "out_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 100)               50100     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 51,110\nTrainable params: 51,110\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "\"\"\" Keras implementation\n",
    "\n",
    "1. input_dim versus input_shape\n",
    "Instead of input_dim also can be used input_shape but as a tuple instead of \n",
    "an integer. Example:\n",
    "\n",
    "    model_tf.add(Dense(H, activation='relu', use_bias=True, input_dim=N))\n",
    "    model_tf.add(Dense(O, activation='softmax', use_bias=True, input_dim=O))\n",
    "\n",
    "`units` or first parameters is the output\n",
    "\n",
    "2. `output` not necessary after first declaration. \n",
    "Instead of:\n",
    "\n",
    "  model_tf.add(Dense(units=O, activation='softmax', use_bias=True, \n",
    "               input_shape=(O,) ))\n",
    "\n",
    "Use this:\n",
    "\n",
    "   model_tf.add(Dense(units=O, activation='softmax', use_bias=True))             \n",
    "\n",
    "3. Unnecessary:\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    Replace with: tf.global_variables_initializer\n",
    "\n",
    "See: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\n",
    "\"\"\"\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.initialize_all_variables())\n",
    "tf.global_variables_initializer\n",
    "\n",
    "model_tf = tf.keras.Sequential()\n",
    "model_tf.add(Dense(units=H, activation='relu', use_bias=True, \n",
    "              input_shape=(N,) ))\n",
    "\n",
    "model_tf.add(Dense(units=O, activation='softmax', use_bias=True))             \n",
    "    \n",
    "def ffpass_tf(x):\n",
    "    xr = x.reshape((1, x.size))\n",
    "    return model_tf.predict(xr)[0]\n",
    "\n",
    "model_tf.summary()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Sequential(\n  (0): Linear(in_features=500, out_features=100, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=100, out_features=10, bias=True)\n  (3): Softmax(dim=1)\n)\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                  [-1, 100]          50,100\n              ReLU-2                  [-1, 100]               0\n            Linear-3                   [-1, 10]           1,010\n           Softmax-4                   [-1, 10]               0\n================================================================\nTotal params: 51,110\nTrainable params: 51,110\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.19\nEstimated Total Size (MB): 0.20\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "model_pt = nn.Sequential(nn.Linear(N, H),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(H, O),\n",
    "                      nn.Softmax(dim=1)\n",
    "                      )\n",
    "\n",
    "\n",
    "def ffpass_torch(x):\n",
    "    xr = x.reshape((1, x.size))\n",
    "    # xr = torch.tensor(xr, dtype=torch.float32)\n",
    "    xr = torch.from_numpy(xr).float()\n",
    "    return model_pt(xr)\n",
    "\n",
    "print(model_pt)\n",
    "summary(model_pt, input_size=(N,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflow \n",
    "### Assign weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(500, 100)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before assignment\n",
    "w0_tf = model_tf.get_weights()[0]\n",
    "w0_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weights and biases to model\n",
    "model_tf.get_layer(index=0).set_weights([w1, b1])\n",
    "model_tf.get_layer(index=1).set_weights([w2, b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.random((N,))\n",
    "# x0 /= sum(x0)\n",
    "\n",
    "out_np    = ffpass_np(x0)\n",
    "out_keras = ffpass_tf(x0)\n",
    "\n",
    "np.allclose(out_np, out_keras, 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get weights from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "310.53552\n-24.158245\n"
    }
   ],
   "source": [
    "# first set of weights\n",
    "w0_tf = model_tf.get_weights()[0]\n",
    "w0_tf.shape\n",
    "print(w0_tf.sum())\n",
    "w2_tf = model_tf.get_weights()[2]\n",
    "print(w2_tf.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorFlow returns the weights as an `500x100` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\nTrue\n"
    }
   ],
   "source": [
    "print(np.allclose(model_tf.get_weights()[0], w1))\n",
    "print(np.allclose(model_tf.get_weights()[2], w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get biases from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\nTrue\n"
    }
   ],
   "source": [
    "# b2 is equal to model.get_weights()[3]\n",
    "print(np.allclose(model_tf.get_weights()[1], b1))\n",
    "print(np.allclose(model_tf.get_weights()[3], b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "310.53552\n-24.158245\n20.29108\n-0.105651826\n"
    }
   ],
   "source": [
    "print(model_tf.get_weights()[0].sum())\n",
    "print(model_tf.get_weights()[2].sum())\n",
    "print(model_tf.get_weights()[1].sum())\n",
    "print(model_tf.get_weights()[3].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch\n",
    "\n",
    "### Assign weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pt.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([100, 500])\ntorch.Size([10, 100])\ntorch.Size([100])\ntorch.Size([10])\n"
    }
   ],
   "source": [
    "print(model_pt.state_dict()['0.weight'].shape)\n",
    "print(model_pt.state_dict()['2.weight'].shape)\n",
    "print(model_pt.state_dict()['0.bias'].shape)\n",
    "print(model_pt.state_dict()['2.bias'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([500, 100])\ntorch.Size([100, 500])\nTrue\n"
    }
   ],
   "source": [
    "# transform initial weight from numpy to tensor\n",
    "w1_pt_t = torch.from_numpy(w1).float()\n",
    "print(w1_pt_t.shape)  # torch.Size([500, 100])\n",
    "# not the sane as weight shape in the model\n",
    "model_pt.state_dict()['0.weight'].shape\n",
    "# transpose w0_pt_t\n",
    "w1_pt_tt = torch.transpose(w1_pt_t, 1, 0)\n",
    "w1_pt_tt = torch.transpose(w1_pt_t, 0, 1)\n",
    "print(w1_pt_tt.shape)\n",
    "\n",
    "# is it now the same shape as model weights\n",
    "print(model_pt.state_dict()['0.weight'].shape == w1_pt_tt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([100, 500])\n"
    },
    {
     "data": {
      "text/plain": "tensor([ 22.8438,  12.5663,  -4.4961,  15.9776,  16.3610,   1.6605,  31.5816,\n        -24.2807,  11.3892,  33.1832, -17.7031,  12.2675,  36.2100,  14.4991,\n          5.7659, -20.2631,  24.5781,  -0.6123, -30.3839, -22.6307,  15.0225,\n        -23.4310,  -8.9231, -71.4337,  12.9205,  -2.0020, -13.5540,  30.6130,\n          3.8229,  18.2278,   0.6141,  52.4337,  -0.4741,  -2.1770,   9.8585,\n         15.1644,  37.5307, -27.1223, -14.3779,  49.4858, -15.1746, -27.9507,\n        -15.6749,  13.1125, -29.7577,  51.3295,  -9.8607, -14.1429,  15.4918,\n         26.2938,  15.4510,  21.1130,  -8.8056, -12.9031,  -9.6456,   4.7877,\n         -7.5941, -24.7822,  -1.2331,  21.0113, -67.8460,   7.8663,  49.2412,\n        -11.1162,   3.3012,  -9.0628,   2.4740, -11.6921,   1.1180, -15.9931,\n         51.8490,   0.5338,  -0.7061,  -5.2812,  27.6975,  26.2964,   2.2344,\n         -6.9736, -16.8031, -36.6282, -35.9391,  31.9341,  -7.7523,  12.6896,\n         -9.4399, -10.7352,  28.9524,  24.7135,   1.3597,  44.4788,  22.3937,\n         11.0537,   2.0492, -53.4721,   0.1595,   8.1793,  12.2601, -24.1439,\n         47.4767,  22.0307])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(w1_pt_tt.shape)\n",
    "w1_pt_tt.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.0856,  0.6421,  0.7033,  ...,  1.0819,  1.6859, -1.8758],\n        [ 0.9973, -1.9779, -0.5981,  ...,  0.5102,  0.3307, -0.5001],\n        [ 0.2830,  0.7123,  2.2007,  ..., -0.0393, -1.3596, -0.4552],\n        ...,\n        [-1.3635, -0.1109,  0.4157,  ..., -0.6561, -0.1741, -0.4237],\n        [ 0.3794, -0.3413,  0.1605,  ..., -0.5363,  0.1477, -1.0752],\n        [-0.3792, -0.2179,  0.8198,  ..., -0.8757, -0.3540, -1.4866]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the input weight #1 to the model\n",
    "model_pt.state_dict()['0.weight'].copy_(w1_pt_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[-1.0856,  0.6421,  0.7033,  ...,  1.0819,  1.6859, -1.8758],\n        [ 0.9973, -1.9779, -0.5981,  ...,  0.5102,  0.3307, -0.5001],\n        [ 0.2830,  0.7123,  2.2007,  ..., -0.0393, -1.3596, -0.4552],\n        ...,\n        [-1.3635, -0.1109,  0.4157,  ..., -0.6561, -0.1741, -0.4237],\n        [ 0.3794, -0.3413,  0.1605,  ..., -0.5363,  0.1477, -1.0752],\n        [-0.3792, -0.2179,  0.8198,  ..., -0.8757, -0.3540, -1.4866]])\n"
    }
   ],
   "source": [
    "# did it get assigned to the model?\n",
    "print(model_pt.state_dict()['0.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 100])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_pt_tt = torch.transpose(torch.from_numpy(w2).float(), 1, 0)\n",
    "w2_pt_tt = torch.transpose(torch.from_numpy(w2).float(), 0, 1)\n",
    "w2_pt_tt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.1129e+00, -5.1684e-02,  1.5901e+00, -5.2124e-01, -8.7518e-01,\n          8.5238e-01,  5.2425e-02, -1.0278e+00, -1.1003e+00,  3.6948e-01,\n          6.5560e-01,  2.6299e-01,  1.0152e-01, -1.1134e+00, -3.2101e-01,\n          8.5195e-01, -1.6749e+00,  2.0671e-01,  3.5323e-01,  1.5109e+00,\n         -1.3764e+00,  4.9770e-01,  1.1479e+00,  2.0785e+00, -8.8207e-01,\n         -3.3461e-01, -1.2842e+00, -2.6498e+00,  1.2119e-01,  7.4749e-01,\n          6.5733e-01, -2.6226e+00, -2.1981e-01,  7.2719e-01,  6.4081e-01,\n          2.5889e+00, -1.1228e+00,  3.8816e-01, -2.8974e-01,  5.0304e-01,\n         -3.4724e-02,  5.7725e-01,  1.9024e+00, -4.8744e-01,  6.6559e-01,\n         -7.3430e-01, -7.7303e-01, -6.2408e-01, -1.1740e-01, -1.9652e+00,\n         -1.7360e-01, -1.4965e-02, -1.3572e+00, -1.0593e+00,  1.1157e+00,\n          9.3877e-01,  2.5633e-01,  1.9308e+00, -7.7957e-01, -5.0439e-01,\n         -6.0820e-01,  8.4222e-01,  1.1120e-01,  8.0944e-02,  4.0295e-01,\n         -1.0022e-01, -4.6749e-01, -9.0825e-01,  1.2442e+00,  1.7009e+00,\n         -1.3515e+00, -1.5828e+00,  7.5882e-01, -6.4562e-01,  4.1731e-01,\n         -4.4093e-01,  1.1315e+00,  3.4546e-01, -1.1185e-01,  1.5268e+00,\n         -1.9816e+00,  3.1182e-01,  2.2611e-01,  4.0182e-02, -6.0922e-01,\n          2.7173e+00, -2.0583e+00, -2.6075e+00,  1.2386e+00, -2.1227e-01,\n         -1.5668e+00, -5.4492e-01, -8.8366e-01, -1.6438e+00, -3.6287e-01,\n         -2.2626e+00,  7.6325e-01, -6.1996e-02,  5.7816e-01,  3.9407e-01],\n        [-6.9418e-01, -1.2634e-01,  2.0633e+00,  8.4497e-01, -3.5964e-01,\n          1.1598e+00, -5.7993e-01,  4.3182e-01,  1.8529e-01, -4.2510e-02,\n         -1.2088e+00,  6.8572e-01,  3.6277e-01, -1.2567e+00,  1.2913e+00,\n          5.5625e-01, -1.1470e+00,  9.2076e-01, -1.0387e+00, -1.1263e+00,\n         -1.3934e+00, -2.5446e+00, -1.7441e+00, -8.4560e-01,  1.0753e+00,\n          7.3147e-01, -5.1432e-01,  4.6084e-01, -2.4130e-01,  5.1954e-01,\n         -4.6475e-01, -1.2389e+00, -2.6986e-01,  9.4108e-02,  4.4837e-01,\n         -9.7917e-01,  7.4680e-01,  1.6496e+00, -1.5761e-01,  3.1499e-01,\n          3.8841e-01,  7.2678e-01,  6.9304e-01, -1.3634e+00,  2.2356e-02,\n         -1.1184e+00, -1.3729e-01, -7.0042e-01,  1.1366e+00,  9.6089e-02,\n         -5.8919e-01, -9.3636e-01, -4.1988e-01, -1.1218e+00,  2.5764e-01,\n         -2.2621e-01,  1.6933e+00,  1.6418e+00, -5.2584e-01, -1.3340e+00,\n          9.2384e-01, -2.2489e-01,  6.9084e-01,  3.0223e-02,  1.3180e+00,\n          1.6623e-01,  2.0517e-01,  2.0729e-01,  5.2094e-01,  5.6313e-01,\n          1.7367e+00,  1.4403e+00, -8.9422e-01,  2.5616e+00,  2.6292e-01,\n          4.6381e-01, -7.4040e-01, -5.6404e-01, -1.2657e+00,  1.8016e-01,\n          1.5211e+00, -1.6109e+00,  1.7167e-01, -1.3175e+00, -3.2265e-01,\n          5.4448e-01,  3.3403e-01,  8.2463e-02, -1.1123e+00,  1.0485e+00,\n         -3.9342e-01,  6.6847e-01,  2.7717e-01,  9.8996e-01, -5.2401e-01,\n         -7.3589e-01, -6.1088e-01,  3.3182e-01,  8.4084e-01,  1.5099e+00],\n        [-4.8984e-01,  3.3453e-01,  1.2138e+00, -1.2793e-01,  3.2575e-01,\n          1.8873e+00,  1.9898e-01, -9.4913e-02, -5.2741e-01, -1.5044e+00,\n         -6.9753e-01,  1.0590e+00,  5.2502e-01, -3.8367e-01, -8.2855e-01,\n          8.8052e-03, -1.1745e+00,  1.1179e+00, -8.9788e-01, -2.7730e-01,\n          1.2507e+00,  1.7807e-01,  9.0370e-01,  1.9003e-02, -2.8669e-01,\n         -2.3686e+00,  1.2997e-01, -1.0423e+00,  5.8816e-01, -6.3994e-01,\n         -1.7334e+00, -5.9743e-01,  2.7156e-01,  1.0639e-01, -1.0598e+00,\n          9.8667e-01,  2.8189e-01, -1.4682e-01, -2.5701e+00,  6.0327e-01,\n         -2.0727e+00,  1.0356e+00,  3.5362e-01,  8.5688e-01,  7.2562e-01,\n          9.9054e-01, -8.0247e-01, -5.3620e-01, -1.1298e-01, -9.5629e-01,\n         -3.7608e-01, -3.4362e-01, -5.9916e-01,  5.4730e-01,  4.9820e-01,\n         -1.2639e+00, -9.9805e-02, -2.3368e+00, -1.0988e+00,  1.4518e+00,\n          3.7568e-01, -2.7703e-01,  3.1627e-01,  2.1260e+00,  1.0606e+00,\n         -1.7602e-01,  6.9570e-01, -2.6833e-01,  3.2194e-01, -2.7050e-03,\n         -1.9151e-02,  1.0282e-01, -8.4406e-01,  6.4162e-01,  1.9459e-01,\n         -1.7362e+00,  7.9569e-01, -1.5559e+00, -1.0391e+00,  2.0776e-01,\n         -3.6239e-02, -1.7906e+00, -1.1783e-01, -1.9843e-01,  1.5597e+00,\n         -7.8659e-01, -2.3848e+00, -4.3065e-01, -8.7740e-01, -4.4630e-01,\n         -1.4819e+00, -2.6199e-01,  1.2279e+00,  5.4013e-01, -1.8330e+00,\n          5.4983e-01, -1.1396e+00,  2.8333e-01,  2.5860e-01, -2.6362e+00],\n        [ 8.6399e-01, -2.6052e+00, -2.0216e+00,  3.6907e-01,  8.6237e-01,\n         -1.4259e+00,  8.4098e-01, -5.0166e-02, -3.0848e-01, -2.6934e+00,\n          3.6176e-01, -1.9376e-01, -9.2292e-01,  1.2429e+00, -9.4374e-01,\n         -4.5948e-01,  2.9404e-01, -9.5776e-01, -4.9511e-01,  6.0564e-01,\n         -4.5506e-01, -1.1824e+00,  1.8232e+00, -1.8307e-02, -2.9675e-01,\n          1.5508e+00, -1.6346e-01,  7.7903e-01, -1.1385e+00,  4.9759e-01,\n         -9.9481e-01,  1.2541e-01,  4.7412e-01, -5.8027e-01, -2.5200e+00,\n          1.6177e+00, -1.3589e+00, -5.1074e-01, -2.0958e+00, -4.1147e-01,\n          1.0077e+00,  1.2727e+00,  1.7524e+00,  9.8353e-01,  7.4537e-01,\n          4.6840e-01, -1.3359e+00,  1.5902e+00, -9.7469e-01, -1.4262e+00,\n         -3.0330e-01, -7.8195e-01,  2.4259e-01, -9.3247e-01,  2.9473e-01,\n         -7.9068e-01,  7.9235e-01, -1.3638e+00,  5.9037e-01,  2.1648e+00,\n         -4.1577e-01, -3.9368e-01,  1.3910e-01,  6.3582e-01, -1.2182e+00,\n          5.7279e-01,  3.6244e-01, -2.6103e-02,  5.5162e-01,  1.9624e+00,\n          1.2794e-01, -3.0658e-01, -1.3851e+00, -1.0103e+00, -4.7332e-01,\n          1.2997e+00,  2.2572e+00, -5.5127e-02,  2.6715e+00, -1.0431e+00,\n         -4.2916e-01, -1.2983e-01, -3.5510e-01, -2.1716e-01,  4.5257e-01,\n          1.0181e-01, -1.8928e+00,  5.9698e-01, -5.7014e-01, -6.0504e-01,\n          2.2055e+00,  8.0067e-01,  9.8819e-01, -4.3104e-01,  1.8693e-01,\n          9.3224e-01,  3.4860e-01, -1.3572e-01,  9.9411e-01,  2.7536e-01],\n        [-3.7427e-01, -1.0094e+00, -1.5472e+00, -3.4725e-01,  5.5072e-01,\n          4.5653e-01,  1.4832e+00,  2.1972e+00,  1.6287e+00,  1.6628e+00,\n         -1.7274e+00,  2.8931e-02, -3.4251e-01, -1.5221e+00, -8.3940e-01,\n          2.6137e-01, -2.1918e+00,  1.5117e+00,  3.4213e-01, -1.9556e-01,\n          8.1351e-02, -1.0439e+00,  1.6708e-01,  2.8144e-01, -7.0225e-01,\n          1.2655e-01, -8.1428e-01, -5.0088e-01,  3.1569e-02,  5.9329e-01,\n          2.0088e+00,  6.5341e-01, -1.0616e-01, -1.8051e-01,  3.8417e-01,\n          1.5101e+00, -5.7727e-01,  1.9436e+00,  1.9319e+00,  1.5180e+00,\n          1.1296e-01,  3.1387e-01, -8.4120e-01,  1.1289e+00,  6.1275e-02,\n          2.3100e-01,  1.2802e+00,  2.1712e+00,  2.8318e+00, -4.3572e-01,\n         -1.9692e-01, -4.6983e-01, -3.6905e-01,  1.3858e-01,  1.1790e+00,\n         -1.3017e+00,  1.8939e+00, -1.4232e+00, -4.3910e-01,  1.0437e+00,\n          3.6969e-01, -8.8936e-01,  8.1215e-01, -3.6931e-01,  7.6593e-01,\n         -7.1973e-02, -1.0148e+00,  9.5473e-01, -1.3360e+00, -9.1681e-01,\n         -8.3041e-03, -6.6561e-01,  1.1514e+00, -1.6200e-01,  1.1370e+00,\n         -9.3499e-01,  2.5241e-01, -2.3422e-01,  1.6618e+00,  9.9554e-01,\n         -1.3245e+00, -1.2391e+00,  1.2411e+00,  4.9403e-01,  1.6749e+00,\n         -1.5781e+00,  1.8818e+00, -1.7939e-01, -2.4438e+00, -5.0163e-01,\n         -3.9082e-01, -1.1184e+00, -4.5217e-01, -9.1899e-01,  1.0857e+00,\n          1.0663e+00,  8.6104e-01, -8.5164e-01, -5.3683e-01, -1.8894e-01],\n        [-1.1155e+00,  3.7852e-01,  1.0503e+00,  8.3559e-01,  6.3435e-02,\n         -1.2021e+00,  1.1201e+00, -1.7721e+00,  7.6319e-01, -9.4650e-01,\n          2.8953e-01,  5.6678e-01, -5.3045e-01,  2.6985e-01,  3.3725e-01,\n          1.1775e+00,  1.4954e+00, -3.1933e-01, -9.8074e-01, -3.3194e-01,\n         -7.2613e-01, -8.4922e-01, -4.0310e-01, -1.0368e+00,  2.9852e-01,\n         -7.5784e-01,  3.1990e-01,  5.1280e-01, -5.3258e-01,  6.0698e-01,\n          2.2954e-01, -7.8965e-01,  1.4920e+00, -8.0759e-03,  4.2917e-01,\n          8.3429e-01, -5.1486e-01, -1.8246e+00,  1.1780e+00,  3.8680e-01,\n          2.8665e-01,  2.6390e-01,  5.6197e-01,  1.1805e+00,  6.7837e-02,\n          1.2758e+00,  5.0897e-01,  1.5493e-01,  2.8634e-01, -1.8431e+00,\n         -7.7738e-01, -3.3073e-01, -6.1530e-01, -2.9020e-01, -6.8649e-02,\n          2.0818e-01, -9.8657e-02, -8.9338e-01,  1.3373e-01, -9.1391e-01,\n         -8.0144e-01,  1.8993e-01,  1.4327e+00, -2.0699e-01, -1.3456e-01,\n          1.6337e+00,  6.0538e-01, -5.8013e-01, -6.5851e-01, -7.1323e-01,\n         -1.5670e+00, -2.2919e-01,  1.0667e-01, -3.9115e-01,  3.2697e-01,\n          1.7410e+00, -1.2098e+00,  1.2031e+00,  3.2762e-01,  1.4338e-01,\n         -9.2445e-02, -7.3273e-01, -1.1166e+00, -1.6039e+00,  1.1433e-01,\n         -6.7672e-01,  1.1505e+00,  4.1001e-02,  1.5502e-01,  9.1674e-01,\n         -1.3058e+00, -8.0416e-01,  6.3655e-01,  9.1865e-01, -1.5547e+00,\n         -7.0956e-01, -7.1047e-01,  6.3252e-01,  2.5195e-01, -1.3979e+00],\n        [-1.0063e+00, -6.2518e-01,  1.0974e+00,  1.0025e+00, -1.5702e-01,\n         -9.5181e-01, -1.8852e-01, -1.5006e+00,  4.0238e-01, -2.5301e-01,\n          1.1390e+00, -1.1190e+00, -6.7973e-01,  3.5903e-01,  8.7734e-01,\n         -3.0863e-01,  1.1878e-01, -2.4199e+00, -3.4251e-01, -9.6930e-01,\n         -8.9710e-01, -1.9692e+00, -9.6111e-01, -2.0471e+00, -5.2287e-01,\n         -1.6993e+00, -9.6968e-01, -2.9335e-01, -1.0307e+00,  4.4218e-01,\n         -1.1548e+00,  9.4677e-01,  1.6670e+00,  6.2428e-01,  2.7190e-01,\n          1.2296e-01,  7.5820e-01, -7.0099e-01,  1.0990e+00,  3.8995e-01,\n          1.6424e-01,  1.5049e+00,  1.4153e+00, -4.5643e-02,  4.6487e-01,\n          5.9372e-01, -2.4639e-01, -2.8963e-01, -5.2568e-01,  3.7477e-01,\n         -1.1262e+00, -3.6518e-01, -3.5526e-01, -5.5881e-01,  3.4354e-01,\n          8.8062e-01, -4.9325e-01,  9.6168e-01, -8.9034e-01, -3.4061e-02,\n          9.8423e-01, -4.7246e-01,  9.1921e-01, -4.5764e-01,  8.2467e-01,\n         -4.6402e-01,  1.8703e-01,  4.6602e-01, -1.5508e+00,  1.8418e-01,\n         -8.4759e-01, -1.5730e-01,  1.9243e+00,  5.7875e-01,  4.5515e-01,\n         -1.2115e-01, -1.0616e+00, -2.6533e-01,  1.5498e-01,  7.1952e-01,\n          1.5679e-01,  1.8523e-01,  4.4699e-01, -2.3806e-01, -2.6482e-01,\n          8.9469e-01,  1.7311e+00, -5.1375e-01,  1.5286e+00, -4.5529e-01,\n         -2.1261e+00, -5.3628e-01,  1.6667e+00, -2.4297e-01,  6.3505e-01,\n         -8.7582e-01, -4.6277e-01,  4.4360e-03, -3.5861e-01,  4.6767e-01],\n        [ 1.4061e+00, -1.5594e+00, -1.8886e+00,  1.1169e+00,  4.1512e-01,\n         -1.4458e+00, -1.8404e-01,  4.9347e-01, -2.8106e-01,  1.1319e+00,\n          1.8645e-01,  8.9036e-01,  1.0914e+00,  4.7125e-02, -8.6521e-01,\n         -1.3261e+00,  3.3363e-02, -1.0960e-01, -3.9977e-01, -5.0666e-01,\n          1.8202e-01,  3.4421e-01,  1.4074e+00,  1.2817e+00, -1.3925e+00,\n          1.2391e+00,  1.1710e+00, -1.0066e+00, -7.8380e-01, -9.9048e-01,\n         -7.0844e-01,  2.0451e+00, -4.1663e-01,  5.7113e-01,  1.3235e+00,\n          1.3186e-01, -1.3412e-01,  2.2801e-01,  8.2595e-03,  1.8297e-01,\n          1.4115e+00, -1.6438e+00, -1.5645e+00,  4.5368e-01, -2.9097e-01,\n         -7.0333e-02, -5.7482e-01,  3.5107e-01,  7.6783e-01,  7.0206e-01,\n         -2.1783e+00, -1.6240e-01, -1.7429e+00, -1.2437e+00,  1.6268e-01,\n          9.7865e-01, -9.6881e-01, -1.5816e+00, -7.3247e-01, -1.5152e-01,\n         -1.7229e+00, -2.5581e-02, -6.9782e-01, -4.5122e-02,  1.5273e+00,\n         -1.7344e+00, -1.4022e+00, -1.3261e-01,  5.7921e-01, -7.8795e-01,\n          1.3489e+00, -1.8420e-01, -3.2861e-01,  6.9422e-01,  8.2627e-01,\n         -3.5698e-01, -9.3424e-01,  7.8660e-01,  6.7148e-02,  1.4248e-01,\n         -7.2425e-01,  4.7136e-01,  1.1060e+00,  7.6033e-02,  1.5662e+00,\n          7.9877e-01, -4.1353e-01,  2.1343e+00,  1.8363e+00, -1.3828e-01,\n          1.3413e+00,  3.2490e-02, -2.0812e+00, -9.5386e-01, -1.0482e+00,\n         -6.5601e-01,  3.5174e-01, -5.1431e-01,  7.5602e-01,  5.4172e-01],\n        [ 8.7668e-01,  1.6536e-01,  1.5050e+00, -2.2643e-01, -1.0409e+00,\n          1.0094e+00, -1.3481e+00,  5.6220e-01,  1.5502e-01, -2.3989e-01,\n         -1.2406e+00, -5.9500e-01,  1.5650e+00, -3.1799e+00,  4.2359e-01,\n         -3.7914e-01, -4.2978e-01, -3.4650e-02, -8.6441e-01, -2.6025e-01,\n          7.4737e-01, -7.3526e-01,  2.4834e-01,  2.0197e+00,  3.9161e-01,\n          4.0622e-01, -1.9302e+00, -7.1162e-01,  3.8249e-01,  9.5928e-02,\n         -1.0323e+00, -9.9468e-02, -1.6272e+00, -5.8174e-01, -8.9265e-01,\n          1.6778e+00,  2.7865e-01, -1.0690e+00,  1.2035e+00, -1.2432e+00,\n         -4.5487e-01,  9.5356e-01,  8.9232e-01, -2.1496e-01, -1.5569e+00,\n          3.7899e-01,  1.1215e+00, -2.7220e+00,  1.9193e-01, -4.5691e-01,\n         -6.7507e-01,  8.1020e-01, -6.9085e-01,  4.0312e-01, -7.9672e-01,\n          3.6243e-01,  3.8663e-01, -3.3706e-01, -2.1646e-01, -9.1816e-01,\n         -5.6594e-01,  3.7301e-01,  3.3143e-01,  8.6245e-01, -5.6455e-01,\n          1.7055e+00,  5.1000e-01, -7.7656e-01,  4.8361e-01,  1.1919e+00,\n         -1.4639e+00,  1.6335e-01, -5.0796e-01, -1.6085e-01, -8.2277e-01,\n          1.8099e+00,  9.5651e-02, -2.4371e-01, -1.0016e+00, -8.3434e-01,\n          9.8248e-01, -2.0443e-01, -1.1600e+00,  1.1907e+00,  2.2760e-01,\n         -2.6037e-01,  4.1520e-01,  5.1426e-01,  3.6522e-01,  5.4237e-02,\n          9.7440e-01,  1.2156e+00, -2.3881e-01,  8.5247e-01,  7.0631e-01,\n          1.3571e+00,  4.1794e-01,  4.0801e-01,  9.7695e-01, -5.1799e-01],\n        [-1.2349e+00,  2.7494e-01,  1.6681e+00,  2.6256e-02,  9.6602e-02,\n         -5.7348e-01, -1.2682e+00, -1.4319e-01, -1.2632e-01,  1.4295e+00,\n          9.8040e-01,  1.7824e+00,  2.4719e+00, -8.8448e-01, -4.2735e-01,\n         -1.0598e+00,  8.3639e-01, -3.0712e-01, -2.8666e-01, -1.7785e-01,\n          1.5565e-01, -1.5251e+00,  6.9361e-01, -5.3933e-01, -9.6894e-01,\n          1.2888e+00, -1.1414e+00, -2.5759e+00,  7.4611e-01, -8.4216e-01,\n         -1.8441e-01,  8.2215e-02,  6.2487e-01, -5.9234e-03, -6.3844e-01,\n         -1.2281e+00, -1.3122e+00,  2.1451e+00,  6.2614e-01, -1.6332e+00,\n         -1.6049e+00,  9.2743e-02, -1.5652e+00, -1.1532e+00,  4.2731e-01,\n          9.6446e-01,  7.3511e-01, -9.5683e-01, -5.0433e-02,  8.5334e-01,\n         -1.0942e-01,  2.4298e+00, -4.1982e-01, -1.0978e+00,  7.3958e-01,\n         -9.1562e-01, -1.5883e+00, -1.0333e+00,  3.0391e-01, -3.5632e-01,\n         -2.2034e-03, -9.4903e-01,  3.5967e-01,  5.4942e-01,  8.0919e-01,\n         -1.0331e+00,  8.4442e-01, -9.9124e-01,  7.0786e-01,  4.9027e-01,\n          2.1806e+00, -2.3573e-01,  3.1050e-01,  7.0775e-01,  1.1393e-01,\n         -1.0462e+00, -1.5570e-01,  9.7280e-01, -1.1338e+00,  5.3651e-01,\n          7.6433e-01,  1.8904e+00, -1.6185e-01, -1.7103e+00, -1.8635e-01,\n          7.2738e-01,  1.6905e+00,  1.4279e-01,  8.6242e-01,  3.2749e-01,\n          2.7217e-01, -1.1635e-01, -4.0906e-01,  1.9965e+00,  4.9143e-01,\n         -1.1780e-02, -1.2780e+00,  4.6988e-01, -2.9048e-01,  5.0718e-01]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the input weight #2 to the model\n",
    "model_pt.state_dict()['2.weight'].copy_(w2_pt_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 1.1129e+00, -5.1684e-02,  1.5901e+00, -5.2124e-01, -8.7518e-01,\n          8.5238e-01,  5.2425e-02, -1.0278e+00, -1.1003e+00,  3.6948e-01,\n          6.5560e-01,  2.6299e-01,  1.0152e-01, -1.1134e+00, -3.2101e-01,\n          8.5195e-01, -1.6749e+00,  2.0671e-01,  3.5323e-01,  1.5109e+00,\n         -1.3764e+00,  4.9770e-01,  1.1479e+00,  2.0785e+00, -8.8207e-01,\n         -3.3461e-01, -1.2842e+00, -2.6498e+00,  1.2119e-01,  7.4749e-01,\n          6.5733e-01, -2.6226e+00, -2.1981e-01,  7.2719e-01,  6.4081e-01,\n          2.5889e+00, -1.1228e+00,  3.8816e-01, -2.8974e-01,  5.0304e-01,\n         -3.4724e-02,  5.7725e-01,  1.9024e+00, -4.8744e-01,  6.6559e-01,\n         -7.3430e-01, -7.7303e-01, -6.2408e-01, -1.1740e-01, -1.9652e+00,\n         -1.7360e-01, -1.4965e-02, -1.3572e+00, -1.0593e+00,  1.1157e+00,\n          9.3877e-01,  2.5633e-01,  1.9308e+00, -7.7957e-01, -5.0439e-01,\n         -6.0820e-01,  8.4222e-01,  1.1120e-01,  8.0944e-02,  4.0295e-01,\n         -1.0022e-01, -4.6749e-01, -9.0825e-01,  1.2442e+00,  1.7009e+00,\n         -1.3515e+00, -1.5828e+00,  7.5882e-01, -6.4562e-01,  4.1731e-01,\n         -4.4093e-01,  1.1315e+00,  3.4546e-01, -1.1185e-01,  1.5268e+00,\n         -1.9816e+00,  3.1182e-01,  2.2611e-01,  4.0182e-02, -6.0922e-01,\n          2.7173e+00, -2.0583e+00, -2.6075e+00,  1.2386e+00, -2.1227e-01,\n         -1.5668e+00, -5.4492e-01, -8.8366e-01, -1.6438e+00, -3.6287e-01,\n         -2.2626e+00,  7.6325e-01, -6.1996e-02,  5.7816e-01,  3.9407e-01],\n        [-6.9418e-01, -1.2634e-01,  2.0633e+00,  8.4497e-01, -3.5964e-01,\n          1.1598e+00, -5.7993e-01,  4.3182e-01,  1.8529e-01, -4.2510e-02,\n         -1.2088e+00,  6.8572e-01,  3.6277e-01, -1.2567e+00,  1.2913e+00,\n          5.5625e-01, -1.1470e+00,  9.2076e-01, -1.0387e+00, -1.1263e+00,\n         -1.3934e+00, -2.5446e+00, -1.7441e+00, -8.4560e-01,  1.0753e+00,\n          7.3147e-01, -5.1432e-01,  4.6084e-01, -2.4130e-01,  5.1954e-01,\n         -4.6475e-01, -1.2389e+00, -2.6986e-01,  9.4108e-02,  4.4837e-01,\n         -9.7917e-01,  7.4680e-01,  1.6496e+00, -1.5761e-01,  3.1499e-01,\n          3.8841e-01,  7.2678e-01,  6.9304e-01, -1.3634e+00,  2.2356e-02,\n         -1.1184e+00, -1.3729e-01, -7.0042e-01,  1.1366e+00,  9.6089e-02,\n         -5.8919e-01, -9.3636e-01, -4.1988e-01, -1.1218e+00,  2.5764e-01,\n         -2.2621e-01,  1.6933e+00,  1.6418e+00, -5.2584e-01, -1.3340e+00,\n          9.2384e-01, -2.2489e-01,  6.9084e-01,  3.0223e-02,  1.3180e+00,\n          1.6623e-01,  2.0517e-01,  2.0729e-01,  5.2094e-01,  5.6313e-01,\n          1.7367e+00,  1.4403e+00, -8.9422e-01,  2.5616e+00,  2.6292e-01,\n          4.6381e-01, -7.4040e-01, -5.6404e-01, -1.2657e+00,  1.8016e-01,\n          1.5211e+00, -1.6109e+00,  1.7167e-01, -1.3175e+00, -3.2265e-01,\n          5.4448e-01,  3.3403e-01,  8.2463e-02, -1.1123e+00,  1.0485e+00,\n         -3.9342e-01,  6.6847e-01,  2.7717e-01,  9.8996e-01, -5.2401e-01,\n         -7.3589e-01, -6.1088e-01,  3.3182e-01,  8.4084e-01,  1.5099e+00],\n        [-4.8984e-01,  3.3453e-01,  1.2138e+00, -1.2793e-01,  3.2575e-01,\n          1.8873e+00,  1.9898e-01, -9.4913e-02, -5.2741e-01, -1.5044e+00,\n         -6.9753e-01,  1.0590e+00,  5.2502e-01, -3.8367e-01, -8.2855e-01,\n          8.8052e-03, -1.1745e+00,  1.1179e+00, -8.9788e-01, -2.7730e-01,\n          1.2507e+00,  1.7807e-01,  9.0370e-01,  1.9003e-02, -2.8669e-01,\n         -2.3686e+00,  1.2997e-01, -1.0423e+00,  5.8816e-01, -6.3994e-01,\n         -1.7334e+00, -5.9743e-01,  2.7156e-01,  1.0639e-01, -1.0598e+00,\n          9.8667e-01,  2.8189e-01, -1.4682e-01, -2.5701e+00,  6.0327e-01,\n         -2.0727e+00,  1.0356e+00,  3.5362e-01,  8.5688e-01,  7.2562e-01,\n          9.9054e-01, -8.0247e-01, -5.3620e-01, -1.1298e-01, -9.5629e-01,\n         -3.7608e-01, -3.4362e-01, -5.9916e-01,  5.4730e-01,  4.9820e-01,\n         -1.2639e+00, -9.9805e-02, -2.3368e+00, -1.0988e+00,  1.4518e+00,\n          3.7568e-01, -2.7703e-01,  3.1627e-01,  2.1260e+00,  1.0606e+00,\n         -1.7602e-01,  6.9570e-01, -2.6833e-01,  3.2194e-01, -2.7050e-03,\n         -1.9151e-02,  1.0282e-01, -8.4406e-01,  6.4162e-01,  1.9459e-01,\n         -1.7362e+00,  7.9569e-01, -1.5559e+00, -1.0391e+00,  2.0776e-01,\n         -3.6239e-02, -1.7906e+00, -1.1783e-01, -1.9843e-01,  1.5597e+00,\n         -7.8659e-01, -2.3848e+00, -4.3065e-01, -8.7740e-01, -4.4630e-01,\n         -1.4819e+00, -2.6199e-01,  1.2279e+00,  5.4013e-01, -1.8330e+00,\n          5.4983e-01, -1.1396e+00,  2.8333e-01,  2.5860e-01, -2.6362e+00],\n        [ 8.6399e-01, -2.6052e+00, -2.0216e+00,  3.6907e-01,  8.6237e-01,\n         -1.4259e+00,  8.4098e-01, -5.0166e-02, -3.0848e-01, -2.6934e+00,\n          3.6176e-01, -1.9376e-01, -9.2292e-01,  1.2429e+00, -9.4374e-01,\n         -4.5948e-01,  2.9404e-01, -9.5776e-01, -4.9511e-01,  6.0564e-01,\n         -4.5506e-01, -1.1824e+00,  1.8232e+00, -1.8307e-02, -2.9675e-01,\n          1.5508e+00, -1.6346e-01,  7.7903e-01, -1.1385e+00,  4.9759e-01,\n         -9.9481e-01,  1.2541e-01,  4.7412e-01, -5.8027e-01, -2.5200e+00,\n          1.6177e+00, -1.3589e+00, -5.1074e-01, -2.0958e+00, -4.1147e-01,\n          1.0077e+00,  1.2727e+00,  1.7524e+00,  9.8353e-01,  7.4537e-01,\n          4.6840e-01, -1.3359e+00,  1.5902e+00, -9.7469e-01, -1.4262e+00,\n         -3.0330e-01, -7.8195e-01,  2.4259e-01, -9.3247e-01,  2.9473e-01,\n         -7.9068e-01,  7.9235e-01, -1.3638e+00,  5.9037e-01,  2.1648e+00,\n         -4.1577e-01, -3.9368e-01,  1.3910e-01,  6.3582e-01, -1.2182e+00,\n          5.7279e-01,  3.6244e-01, -2.6103e-02,  5.5162e-01,  1.9624e+00,\n          1.2794e-01, -3.0658e-01, -1.3851e+00, -1.0103e+00, -4.7332e-01,\n          1.2997e+00,  2.2572e+00, -5.5127e-02,  2.6715e+00, -1.0431e+00,\n         -4.2916e-01, -1.2983e-01, -3.5510e-01, -2.1716e-01,  4.5257e-01,\n          1.0181e-01, -1.8928e+00,  5.9698e-01, -5.7014e-01, -6.0504e-01,\n          2.2055e+00,  8.0067e-01,  9.8819e-01, -4.3104e-01,  1.8693e-01,\n          9.3224e-01,  3.4860e-01, -1.3572e-01,  9.9411e-01,  2.7536e-01],\n        [-3.7427e-01, -1.0094e+00, -1.5472e+00, -3.4725e-01,  5.5072e-01,\n          4.5653e-01,  1.4832e+00,  2.1972e+00,  1.6287e+00,  1.6628e+00,\n         -1.7274e+00,  2.8931e-02, -3.4251e-01, -1.5221e+00, -8.3940e-01,\n          2.6137e-01, -2.1918e+00,  1.5117e+00,  3.4213e-01, -1.9556e-01,\n          8.1351e-02, -1.0439e+00,  1.6708e-01,  2.8144e-01, -7.0225e-01,\n          1.2655e-01, -8.1428e-01, -5.0088e-01,  3.1569e-02,  5.9329e-01,\n          2.0088e+00,  6.5341e-01, -1.0616e-01, -1.8051e-01,  3.8417e-01,\n          1.5101e+00, -5.7727e-01,  1.9436e+00,  1.9319e+00,  1.5180e+00,\n          1.1296e-01,  3.1387e-01, -8.4120e-01,  1.1289e+00,  6.1275e-02,\n          2.3100e-01,  1.2802e+00,  2.1712e+00,  2.8318e+00, -4.3572e-01,\n         -1.9692e-01, -4.6983e-01, -3.6905e-01,  1.3858e-01,  1.1790e+00,\n         -1.3017e+00,  1.8939e+00, -1.4232e+00, -4.3910e-01,  1.0437e+00,\n          3.6969e-01, -8.8936e-01,  8.1215e-01, -3.6931e-01,  7.6593e-01,\n         -7.1973e-02, -1.0148e+00,  9.5473e-01, -1.3360e+00, -9.1681e-01,\n         -8.3041e-03, -6.6561e-01,  1.1514e+00, -1.6200e-01,  1.1370e+00,\n         -9.3499e-01,  2.5241e-01, -2.3422e-01,  1.6618e+00,  9.9554e-01,\n         -1.3245e+00, -1.2391e+00,  1.2411e+00,  4.9403e-01,  1.6749e+00,\n         -1.5781e+00,  1.8818e+00, -1.7939e-01, -2.4438e+00, -5.0163e-01,\n         -3.9082e-01, -1.1184e+00, -4.5217e-01, -9.1899e-01,  1.0857e+00,\n          1.0663e+00,  8.6104e-01, -8.5164e-01, -5.3683e-01, -1.8894e-01],\n        [-1.1155e+00,  3.7852e-01,  1.0503e+00,  8.3559e-01,  6.3435e-02,\n         -1.2021e+00,  1.1201e+00, -1.7721e+00,  7.6319e-01, -9.4650e-01,\n          2.8953e-01,  5.6678e-01, -5.3045e-01,  2.6985e-01,  3.3725e-01,\n          1.1775e+00,  1.4954e+00, -3.1933e-01, -9.8074e-01, -3.3194e-01,\n         -7.2613e-01, -8.4922e-01, -4.0310e-01, -1.0368e+00,  2.9852e-01,\n         -7.5784e-01,  3.1990e-01,  5.1280e-01, -5.3258e-01,  6.0698e-01,\n          2.2954e-01, -7.8965e-01,  1.4920e+00, -8.0759e-03,  4.2917e-01,\n          8.3429e-01, -5.1486e-01, -1.8246e+00,  1.1780e+00,  3.8680e-01,\n          2.8665e-01,  2.6390e-01,  5.6197e-01,  1.1805e+00,  6.7837e-02,\n          1.2758e+00,  5.0897e-01,  1.5493e-01,  2.8634e-01, -1.8431e+00,\n         -7.7738e-01, -3.3073e-01, -6.1530e-01, -2.9020e-01, -6.8649e-02,\n          2.0818e-01, -9.8657e-02, -8.9338e-01,  1.3373e-01, -9.1391e-01,\n         -8.0144e-01,  1.8993e-01,  1.4327e+00, -2.0699e-01, -1.3456e-01,\n          1.6337e+00,  6.0538e-01, -5.8013e-01, -6.5851e-01, -7.1323e-01,\n         -1.5670e+00, -2.2919e-01,  1.0667e-01, -3.9115e-01,  3.2697e-01,\n          1.7410e+00, -1.2098e+00,  1.2031e+00,  3.2762e-01,  1.4338e-01,\n         -9.2445e-02, -7.3273e-01, -1.1166e+00, -1.6039e+00,  1.1433e-01,\n         -6.7672e-01,  1.1505e+00,  4.1001e-02,  1.5502e-01,  9.1674e-01,\n         -1.3058e+00, -8.0416e-01,  6.3655e-01,  9.1865e-01, -1.5547e+00,\n         -7.0956e-01, -7.1047e-01,  6.3252e-01,  2.5195e-01, -1.3979e+00],\n        [-1.0063e+00, -6.2518e-01,  1.0974e+00,  1.0025e+00, -1.5702e-01,\n         -9.5181e-01, -1.8852e-01, -1.5006e+00,  4.0238e-01, -2.5301e-01,\n          1.1390e+00, -1.1190e+00, -6.7973e-01,  3.5903e-01,  8.7734e-01,\n         -3.0863e-01,  1.1878e-01, -2.4199e+00, -3.4251e-01, -9.6930e-01,\n         -8.9710e-01, -1.9692e+00, -9.6111e-01, -2.0471e+00, -5.2287e-01,\n         -1.6993e+00, -9.6968e-01, -2.9335e-01, -1.0307e+00,  4.4218e-01,\n         -1.1548e+00,  9.4677e-01,  1.6670e+00,  6.2428e-01,  2.7190e-01,\n          1.2296e-01,  7.5820e-01, -7.0099e-01,  1.0990e+00,  3.8995e-01,\n          1.6424e-01,  1.5049e+00,  1.4153e+00, -4.5643e-02,  4.6487e-01,\n          5.9372e-01, -2.4639e-01, -2.8963e-01, -5.2568e-01,  3.7477e-01,\n         -1.1262e+00, -3.6518e-01, -3.5526e-01, -5.5881e-01,  3.4354e-01,\n          8.8062e-01, -4.9325e-01,  9.6168e-01, -8.9034e-01, -3.4061e-02,\n          9.8423e-01, -4.7246e-01,  9.1921e-01, -4.5764e-01,  8.2467e-01,\n         -4.6402e-01,  1.8703e-01,  4.6602e-01, -1.5508e+00,  1.8418e-01,\n         -8.4759e-01, -1.5730e-01,  1.9243e+00,  5.7875e-01,  4.5515e-01,\n         -1.2115e-01, -1.0616e+00, -2.6533e-01,  1.5498e-01,  7.1952e-01,\n          1.5679e-01,  1.8523e-01,  4.4699e-01, -2.3806e-01, -2.6482e-01,\n          8.9469e-01,  1.7311e+00, -5.1375e-01,  1.5286e+00, -4.5529e-01,\n         -2.1261e+00, -5.3628e-01,  1.6667e+00, -2.4297e-01,  6.3505e-01,\n         -8.7582e-01, -4.6277e-01,  4.4360e-03, -3.5861e-01,  4.6767e-01],\n        [ 1.4061e+00, -1.5594e+00, -1.8886e+00,  1.1169e+00,  4.1512e-01,\n         -1.4458e+00, -1.8404e-01,  4.9347e-01, -2.8106e-01,  1.1319e+00,\n          1.8645e-01,  8.9036e-01,  1.0914e+00,  4.7125e-02, -8.6521e-01,\n         -1.3261e+00,  3.3363e-02, -1.0960e-01, -3.9977e-01, -5.0666e-01,\n          1.8202e-01,  3.4421e-01,  1.4074e+00,  1.2817e+00, -1.3925e+00,\n          1.2391e+00,  1.1710e+00, -1.0066e+00, -7.8380e-01, -9.9048e-01,\n         -7.0844e-01,  2.0451e+00, -4.1663e-01,  5.7113e-01,  1.3235e+00,\n          1.3186e-01, -1.3412e-01,  2.2801e-01,  8.2595e-03,  1.8297e-01,\n          1.4115e+00, -1.6438e+00, -1.5645e+00,  4.5368e-01, -2.9097e-01,\n         -7.0333e-02, -5.7482e-01,  3.5107e-01,  7.6783e-01,  7.0206e-01,\n         -2.1783e+00, -1.6240e-01, -1.7429e+00, -1.2437e+00,  1.6268e-01,\n          9.7865e-01, -9.6881e-01, -1.5816e+00, -7.3247e-01, -1.5152e-01,\n         -1.7229e+00, -2.5581e-02, -6.9782e-01, -4.5122e-02,  1.5273e+00,\n         -1.7344e+00, -1.4022e+00, -1.3261e-01,  5.7921e-01, -7.8795e-01,\n          1.3489e+00, -1.8420e-01, -3.2861e-01,  6.9422e-01,  8.2627e-01,\n         -3.5698e-01, -9.3424e-01,  7.8660e-01,  6.7148e-02,  1.4248e-01,\n         -7.2425e-01,  4.7136e-01,  1.1060e+00,  7.6033e-02,  1.5662e+00,\n          7.9877e-01, -4.1353e-01,  2.1343e+00,  1.8363e+00, -1.3828e-01,\n          1.3413e+00,  3.2490e-02, -2.0812e+00, -9.5386e-01, -1.0482e+00,\n         -6.5601e-01,  3.5174e-01, -5.1431e-01,  7.5602e-01,  5.4172e-01],\n        [ 8.7668e-01,  1.6536e-01,  1.5050e+00, -2.2643e-01, -1.0409e+00,\n          1.0094e+00, -1.3481e+00,  5.6220e-01,  1.5502e-01, -2.3989e-01,\n         -1.2406e+00, -5.9500e-01,  1.5650e+00, -3.1799e+00,  4.2359e-01,\n         -3.7914e-01, -4.2978e-01, -3.4650e-02, -8.6441e-01, -2.6025e-01,\n          7.4737e-01, -7.3526e-01,  2.4834e-01,  2.0197e+00,  3.9161e-01,\n          4.0622e-01, -1.9302e+00, -7.1162e-01,  3.8249e-01,  9.5928e-02,\n         -1.0323e+00, -9.9468e-02, -1.6272e+00, -5.8174e-01, -8.9265e-01,\n          1.6778e+00,  2.7865e-01, -1.0690e+00,  1.2035e+00, -1.2432e+00,\n         -4.5487e-01,  9.5356e-01,  8.9232e-01, -2.1496e-01, -1.5569e+00,\n          3.7899e-01,  1.1215e+00, -2.7220e+00,  1.9193e-01, -4.5691e-01,\n         -6.7507e-01,  8.1020e-01, -6.9085e-01,  4.0312e-01, -7.9672e-01,\n          3.6243e-01,  3.8663e-01, -3.3706e-01, -2.1646e-01, -9.1816e-01,\n         -5.6594e-01,  3.7301e-01,  3.3143e-01,  8.6245e-01, -5.6455e-01,\n          1.7055e+00,  5.1000e-01, -7.7656e-01,  4.8361e-01,  1.1919e+00,\n         -1.4639e+00,  1.6335e-01, -5.0796e-01, -1.6085e-01, -8.2277e-01,\n          1.8099e+00,  9.5651e-02, -2.4371e-01, -1.0016e+00, -8.3434e-01,\n          9.8248e-01, -2.0443e-01, -1.1600e+00,  1.1907e+00,  2.2760e-01,\n         -2.6037e-01,  4.1520e-01,  5.1426e-01,  3.6522e-01,  5.4237e-02,\n          9.7440e-01,  1.2156e+00, -2.3881e-01,  8.5247e-01,  7.0631e-01,\n          1.3571e+00,  4.1794e-01,  4.0801e-01,  9.7695e-01, -5.1799e-01],\n        [-1.2349e+00,  2.7494e-01,  1.6681e+00,  2.6256e-02,  9.6602e-02,\n         -5.7348e-01, -1.2682e+00, -1.4319e-01, -1.2632e-01,  1.4295e+00,\n          9.8040e-01,  1.7824e+00,  2.4719e+00, -8.8448e-01, -4.2735e-01,\n         -1.0598e+00,  8.3639e-01, -3.0712e-01, -2.8666e-01, -1.7785e-01,\n          1.5565e-01, -1.5251e+00,  6.9361e-01, -5.3933e-01, -9.6894e-01,\n          1.2888e+00, -1.1414e+00, -2.5759e+00,  7.4611e-01, -8.4216e-01,\n         -1.8441e-01,  8.2215e-02,  6.2487e-01, -5.9234e-03, -6.3844e-01,\n         -1.2281e+00, -1.3122e+00,  2.1451e+00,  6.2614e-01, -1.6332e+00,\n         -1.6049e+00,  9.2743e-02, -1.5652e+00, -1.1532e+00,  4.2731e-01,\n          9.6446e-01,  7.3511e-01, -9.5683e-01, -5.0433e-02,  8.5334e-01,\n         -1.0942e-01,  2.4298e+00, -4.1982e-01, -1.0978e+00,  7.3958e-01,\n         -9.1562e-01, -1.5883e+00, -1.0333e+00,  3.0391e-01, -3.5632e-01,\n         -2.2034e-03, -9.4903e-01,  3.5967e-01,  5.4942e-01,  8.0919e-01,\n         -1.0331e+00,  8.4442e-01, -9.9124e-01,  7.0786e-01,  4.9027e-01,\n          2.1806e+00, -2.3573e-01,  3.1050e-01,  7.0775e-01,  1.1393e-01,\n         -1.0462e+00, -1.5570e-01,  9.7280e-01, -1.1338e+00,  5.3651e-01,\n          7.6433e-01,  1.8904e+00, -1.6185e-01, -1.7103e+00, -1.8635e-01,\n          7.2738e-01,  1.6905e+00,  1.4279e-01,  8.6242e-01,  3.2749e-01,\n          2.7217e-01, -1.1635e-01, -4.0906e-01,  1.9965e+00,  4.9143e-01,\n         -1.1780e-02, -1.2780e+00,  4.6988e-01, -2.9048e-01,  5.0718e-01]])\n"
    }
   ],
   "source": [
    "# did it get assigned to the model?\n",
    "print(model_pt.state_dict()['2.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.9451e+00,  4.0798e-01,  2.1048e+00,  8.0404e-01, -1.3158e+00,\n        -7.6591e-01, -2.7069e-01,  1.6593e-01,  1.0397e+00, -4.8862e-01,\n         6.8756e-01, -1.4139e-02,  1.1679e+00, -1.4800e+00, -6.1002e-01,\n         1.9840e+00, -9.6058e-01,  1.1664e+00, -6.0536e-01,  9.5474e-01,\n         8.2983e-01,  2.3849e+00, -3.7025e-01, -7.8233e-01,  1.4159e-01,\n        -6.7888e-01,  7.7579e-01,  8.7329e-01, -8.4585e-01,  1.6641e+00,\n         5.0839e-01,  1.6293e+00, -8.0582e-01, -1.2374e+00,  1.4533e+00,\n        -1.1955e+00,  4.1556e-01,  1.7001e+00,  6.6412e-01, -9.8218e-01,\n        -4.6998e-01, -1.4745e+00,  4.2507e-01,  5.2342e-02,  1.3419e-01,\n        -1.6378e-01, -3.7818e-01, -3.2270e-01, -1.3531e+00, -7.3084e-01,\n         7.8842e-01,  4.0034e-01,  1.7838e+00, -5.6231e-01,  1.2295e+00,\n         1.6651e+00,  6.6585e-02, -1.3740e-01,  1.1277e+00,  1.2111e+00,\n        -1.5957e-01, -1.1463e-02,  1.8059e+00,  2.8191e-01, -8.0577e-01,\n         4.8367e-01,  7.0699e-01, -8.8523e-02,  2.5755e-01, -9.6363e-01,\n         4.2028e-01, -6.4255e-01,  5.7386e-01,  5.0060e-01, -1.0384e+00,\n         1.1357e+00, -1.3228e+00, -9.1590e-02, -2.9721e-02, -2.0314e+00,\n         8.7201e-01,  6.5236e-01,  2.5265e-02, -3.2738e-01, -3.8746e-01,\n        -1.5964e-01,  9.0671e-01,  1.5982e+00,  7.4395e-01,  5.2370e-01,\n        -9.5638e-01, -9.3864e-01,  1.0137e+00,  3.2110e-04, -1.0284e+00,\n        -1.3855e-01,  7.0598e-01,  2.1898e+00, -8.6392e-02,  7.5038e-01])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.shape\n",
    "torch.from_numpy(b1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-0.1374,  0.5375,  0.3599, -0.1966, -0.0126,  0.2115, -1.2572, -0.0510,\n         0.4954, -0.0551])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_pt_t = torch.from_numpy(b1).float()\n",
    "# assign the input bias #1 to the model\n",
    "model_pt.state_dict()['0.bias'].copy_(b1_pt_t)\n",
    "\n",
    "b2_pt_t = torch.from_numpy(b2).float()\n",
    "# assign the input bias #1 to the model\n",
    "model_pt.state_dict()['2.bias'].copy_(b2_pt_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get weights from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(310.5356)\ntensor(-24.1582)\n"
    }
   ],
   "source": [
    "print(model_pt.state_dict()['0.weight'].sum())\n",
    "print(model_pt.state_dict()['2.weight'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\nTrue\n"
    }
   ],
   "source": [
    "print(np.allclose(model_pt.state_dict()['0.weight'], w1_pt_tt))\n",
    "print(np.allclose(model_pt.state_dict()['2.weight'], w2_pt_tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get biases from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(20.2911)\ntensor(-0.1057)\nTrue\nTrue\n"
    }
   ],
   "source": [
    "print(model_pt.state_dict()['0.bias'].sum())\n",
    "print(model_pt.state_dict()['2.bias'].sum())\n",
    "\n",
    "print(np.allclose(model_pt.state_dict()['0.bias'], b1_pt_t))\n",
    "print(np.allclose(model_pt.state_dict()['2.bias'], b2_pt_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[1.55103949e-145 2.20411161e-056 3.73912060e-089 7.52555573e-082\n 1.80279742e-038 8.51485967e-029 3.01765540e-086 1.00000000e+000\n 1.58494512e-042 3.23202418e-018]\ntensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.8028e-38, 8.5148e-29,\n         0.0000e+00, 1.0000e+00, 1.5849e-42, 3.2320e-18]],\n       grad_fn=<SoftmaxBackward>)\n"
    }
   ],
   "source": [
    "out_np    = ffpass_np(x0)\n",
    "out_torch = ffpass_torch(x0)\n",
    "\n",
    "# np.allclose(out_np, out_keras, 1e-4)\n",
    "print(out_np)\n",
    "print(out_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(310.5356)\ntensor(-24.1582)\ntensor(20.2911)\ntensor(-0.1057)\n"
    }
   ],
   "source": [
    "print(model_pt.state_dict()['0.weight'].sum())\n",
    "print(model_pt.state_dict()['2.weight'].sum())\n",
    "print(model_pt.state_dict()['0.bias'].sum())\n",
    "print(model_pt.state_dict()['2.bias'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffpass_torch(x):\n",
    "    xr = x.reshape((1, x.size))\n",
    "    # xr = torch.tensor(xr, dtype=torch.float32)\n",
    "    xr = torch.from_numpy(xr).float()\n",
    "    return model_pt(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(500,)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 8.0899e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0638e-44,\n         5.3025e-36, 1.0000e+00, 0.0000e+00, 5.0871e-25]],\n       grad_fn=<SoftmaxBackward>)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.random((N,))\n",
    "xr = x0.reshape((1, x0.size))\n",
    "xr = torch.from_numpy(xr).float()\n",
    "# xr = xr.t()\n",
    "xr.shape\n",
    "\n",
    "model_pt(xr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 8.0899e-31, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.0638e-44,\n         5.3025e-36, 1.0000e+00, 0.0000e+00, 5.0871e-25]],\n       grad_fn=<SoftmaxBackward>)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pt.forward(xr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "LR(\n  (linear1): Linear(in_features=500, out_features=100, bias=True)\n  (linear2): Linear(in_features=100, out_features=10, bias=True)\n)\n"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LR(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()  # always\n",
    "    self.linear1 = nn.Linear(N, H)\n",
    "    self.linear2 = nn.Linear(H, O)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.linear1(x))\n",
    "    x = F.softmax(self.linear2(x), dim=1)\n",
    "    return x\n",
    "\n",
    "model = LR()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "odict_keys(['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias'])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(4.6353)\ntensor(-2.1698)\ntensor(0.0626)\ntensor(-0.0637)\n"
    }
   ],
   "source": [
    "print(model.state_dict()['linear1.weight'].sum())\n",
    "print(model.state_dict()['linear2.weight'].sum())\n",
    "print(model.state_dict()['linear1.bias'].sum())\n",
    "print(model.state_dict()['linear2.bias'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_tt = torch.transpose(torch.from_numpy(w1).float(), 0, 1)\n",
    "_ = model.state_dict()['linear1.weight'].copy_(w1_tt)\n",
    "w2_tt = torch.transpose(torch.from_numpy(w2).float(), 0, 1)\n",
    "_ = model.state_dict()['linear2.weight'].copy_(w2_tt)\n",
    "\n",
    "b1_pt_t = torch.from_numpy(b1).float()\n",
    "# assign the input bias #1 to the model\n",
    "model.state_dict()['linear1.bias'].copy_(b1_pt_t)\n",
    "\n",
    "b2_pt_t = torch.from_numpy(b2).float()\n",
    "# assign the input bias #1 to the model\n",
    "_ = model.state_dict()['linear2.bias'].copy_(b2_pt_t)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(310.5356)\ntensor(-24.1582)\ntensor(20.2911)\ntensor(-0.1057)\n"
    }
   ],
   "source": [
    "print(model.state_dict()['linear1.weight'].sum())\n",
    "print(model.state_dict()['linear2.weight'].sum())\n",
    "print(model.state_dict()['linear1.bias'].sum())\n",
    "print(model.state_dict()['linear2.bias'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.0000e+00, 1.4159e-41, 0.0000e+00, 0.0000e+00, 1.0696e-29, 0.0000e+00,\n         0.0000e+00, 1.0000e+00, 0.0000e+00, 1.9758e-43]],\n       grad_fn=<SoftmaxBackward>)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.random.random((N,))\n",
    "x0 = x0.reshape((1, x0.size))\n",
    "x0_t = torch.from_numpy(x0).float()\n",
    "model_x0_t = model(x0_t)\n",
    "model_x0_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing numpy versus model output\n",
    "out_np    = ffpass_np(x0)\n",
    "out_torch = ffpass_torch(x0)\n",
    "\n",
    "np.allclose(out_np, model_x0_t.detach().numpy(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.22034914e-187, 1.41593296e-041, 5.71192629e-129,\n        2.03546147e-106, 1.06955293e-029, 1.26702997e-050,\n        1.82215367e-079, 1.00000000e+000, 3.84671549e-058,\n        1.96980493e-043]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.0000000e+00, 1.4158720e-41, 0.0000000e+00, 0.0000000e+00,\n        1.0695545e-29, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n        0.0000000e+00, 1.9758308e-43]], dtype=float32)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_x0_t.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}