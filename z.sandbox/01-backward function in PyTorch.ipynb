{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-backward function in PyTorch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M6Cqz-R_2voX","colab_type":"text"},"source":["# Explaining the backward function in PyTorch"]},{"cell_type":"markdown","metadata":{"id":"M4BZDXVm2j0a","colab_type":"text"},"source":["**Sources**: \n","* https://stackoverflow.com/questions/57248777/backward-function-in-pytorch\n","\n","* https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95:\n","\n","Backward is the function which actually calculates the gradient by passing its argument, a `1x1` unit tensor by default, through the backward graph all the way up to every leaf node traceable from the calling root tensor.\n","\n","An important thing to notice is that when `z.backward()` is called, a tensor is automatically passed as `z.backward(torch.tensor(1.0))`. The `torch.tensor(1.0)` is the **external gradient** provided to terminate the chain rule gradient multiplications. This **external gradient** is passed as the input to the `MulBackward` function to further calculate the gradient of x. The dimension of the tensor passed into the `.backward()` function must be the same as the dimension of the tensor which gradient is being calculated. For example, if the gradient enabled tensor `x` and `y` are as follows:\n","\n","    x = torch.tensor([0.0, 2.0, 8.0], requires_grad = True)\n","\n","    y = torch.tensor([5.0 , 1.0 , 7.0], requires_grad = True)\n","\n","and \n","\n","    z = x * y\n","\n","then, to calculate the gradients of `z` (a `1x3` tensor), with respect to `x` or `y` , an **external gradient** needs to be passed to `z.backward()` function as follows: \n","\n","    z.backward(torch.FloatTensor([1.0, 1.0, 1.0])\n","\n","If not, `z.backward()` would give a `RuntimeError: grad can be implicitly created only for scalar outputs`"]},{"cell_type":"markdown","metadata":{"id":"4XBeU1-2hUSk","colab_type":"text"},"source":["## Simplest example: Gradient of a scalar\n","PyTorch assigns a unary tensor by default as an argument to `backward`. This only happens with scalars."]},{"cell_type":"code","metadata":{"id":"UtcbVcimhbNY","colab_type":"code","outputId":"67b2a04c-9b58-41ed-efb6-1be55c52469f","executionInfo":{"status":"ok","timestamp":1576261802460,"user_tz":360,"elapsed":417,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import torch\n","from torch.autograd import Variable\n","\n","a = Variable(torch.FloatTensor([4.0]), requires_grad=True) \n","out = a * a\n","out.backward()      # We are not assigning any unary tensor as argument\n","print(a.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([8.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ME979CJdicqt","colab_type":"text"},"source":["This other example (below), shows an implicit assignment of a unary tensor to the `backward` function."]},{"cell_type":"code","metadata":{"id":"_1AFs918iDz3","colab_type":"code","outputId":"8918209f-c1da-437d-96c6-c92d784d76a5","executionInfo":{"status":"ok","timestamp":1576261979867,"user_tz":360,"elapsed":489,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# equivalent example\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","\n","a = Variable(torch.FloatTensor([4.0]), requires_grad=True) \n","out = a * a\n","out.backward(torch.tensor([1.0]))      # assigning a unary tensor as argument \n","print(a.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([8.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qj3pQD6WiyiN","colab_type":"text"},"source":["## Gradient of non-scalar tensors"]},{"cell_type":"code","metadata":{"id":"zUf4Tf-g1SlO","colab_type":"code","outputId":"c2a04cd4-0f36-47ae-d7d5-01aedb137103","executionInfo":{"status":"ok","timestamp":1576216778248,"user_tz":360,"elapsed":496,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import numpy as np\n","import torch\n","from torch.autograd import Variable\n","\n","\n","a = Variable(torch.FloatTensor([[1,2,3], [4,5,6]]), requires_grad=True) \n","out = a * a\n","out.backward(a)\n","print(a.grad)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 2.,  8., 18.],\n","        [32., 50., 72.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UJ4SZ6dc1bRE","colab_type":"text"},"source":["The previous operation is wrong in this simple isolated case (above), because we are assigning non unary weights, which means we won't be able to get the correct results of the derivatives of `a^2`. In other words, we are transforming the output."]},{"cell_type":"markdown","metadata":{"id":"CjHO4foxg5uu","colab_type":"text"},"source":["This other example (below), shows the correct use of a tensor on the `backward` function. It is a tensor of ones."]},{"cell_type":"code","metadata":{"id":"vOHKF_dX1bix","colab_type":"code","outputId":"741c4487-38c9-43a4-9afa-04dbc221b015","executionInfo":{"status":"ok","timestamp":1576216811116,"user_tz":360,"elapsed":606,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import numpy as np\n","import torch\n","from torch.autograd import Variable\n","\n","\n","a = Variable(torch.FloatTensor([[1,2,3],[4,5,6]]), requires_grad=True) \n","\n","# add this tensor for the \"weights\" or external gradient\n","w = torch.FloatTensor([[1,1,1],[1,1,1]])\n","\n","out = a * a\n","out.backward(w)\n","print(a.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 2.,  4.,  6.],\n","        [ 8., 10., 12.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r_eOlKsfS9gR","colab_type":"text"},"source":["## Another example with a 5x3 tensor"]},{"cell_type":"code","metadata":{"id":"fhu2-OQ_TAJK","colab_type":"code","outputId":"9e866c56-0f65-4e13-8a90-1c155b85c59e","executionInfo":{"status":"ok","timestamp":1576258094893,"user_tz":360,"elapsed":5368,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["# this example uses a 5x3 tensor\n","import numpy as np\n","import torch\n","\n","x = np.array([[73, 67, 43],\n","              [91, 88, 64],\n","              [87, 134, 58],\n","              [102, 43, 37],\n","              [69, 96, 70]], dtype='float32')\n","\n","x = torch.from_numpy(x)\n","x.requires_grad = True\n","print(x)\n","\n","# this tensor represents the weights\n","w = torch.ones(5, 3, requires_grad=True) # external gradient\n","\n","y = x**2\n","\n","y.backward(w, retain_graph=True)\n","print(x.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 73.,  67.,  43.],\n","        [ 91.,  88.,  64.],\n","        [ 87., 134.,  58.],\n","        [102.,  43.,  37.],\n","        [ 69.,  96.,  70.]], requires_grad=True)\n","tensor([[146., 134.,  86.],\n","        [182., 176., 128.],\n","        [174., 268., 116.],\n","        [204.,  86.,  74.],\n","        [138., 192., 140.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3LhonYncULMs","colab_type":"text"},"source":["## Gradients with multiple variables\n","\n","Source: https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec"]},{"cell_type":"markdown","metadata":{"id":"bN5MPFb4WASH","colab_type":"text"},"source":["![alt text](https://drive.google.com/uc?id=1adhiJx8-y9ALXBfwKl0UzQOy-DjHUecm)\n"]},{"cell_type":"code","metadata":{"id":"-6HcE0kFT86Z","colab_type":"code","outputId":"58daed6a-9230-41bc-eaa1-83c904f3ca2b","executionInfo":{"status":"ok","timestamp":1576258327576,"user_tz":360,"elapsed":412,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["from torch import FloatTensor\n","from torch.autograd import Variable\n","\n","\n","# Define the leaf nodes\n","a = Variable(FloatTensor([4]))\n","\n","weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n","\n","# unpack the weights for nicer assignment\n","w1, w2, w3, w4 = weights\n","\n","b = w1 * a\n","c = w2 * a\n","d = w3 * b + w4 * c\n","L = (10 - d)\n","\n","L.backward()\n","\n","for index, weight in enumerate(weights, start=1):\n","    gradient, *_ = weight.grad.data\n","    print(f\"Gradient of w{index} w.r.t to L: {gradient}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Gradient of w1 w.r.t to L: -36.0\n","Gradient of w2 w.r.t to L: -28.0\n","Gradient of w3 w.r.t to L: -8.0\n","Gradient of w4 w.r.t to L: -20.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JIU1U07DXy1x","colab_type":"text"},"source":["![alt text](https://drive.google.com/uc?id=1jUhObd5fpqzgjQnZ_l4pa7g1gfkjcL0U)"]},{"cell_type":"markdown","metadata":{"id":"QMHlddZHcasT","colab_type":"text"},"source":["## Example. Get the size of the independent variable"]},{"cell_type":"code","metadata":{"id":"vxdBd0LOUBeF","colab_type":"code","outputId":"ff4f87f9-18e7-4d86-aa54-6d5680d1eb17","executionInfo":{"status":"ok","timestamp":1576260441013,"user_tz":360,"elapsed":454,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["# From an example on linear regression\n","x = np.arange(1,16,1)\n","x = torch.from_numpy(x)\n","x = x.float()            # torch.Size([15])\n","print(x.shape)\n","x = x.view(-1, 3)        # transform tensor to a 5x3 \n","x.requires_grad = True   # torch.Size([5, 3])\n","print(x)\n","\n","# w represents the weights and y the model. very, very simple example\n","# using size of x, we ensure w has the same size as x\n","w = torch.ones((x.size()[0], x.size()[1]), requires_grad = True)\n","y = x**2\n","\n","y.backward(w)\n","print(x.grad)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["torch.Size([15])\n","tensor([[ 1.,  2.,  3.],\n","        [ 4.,  5.,  6.],\n","        [ 7.,  8.,  9.],\n","        [10., 11., 12.],\n","        [13., 14., 15.]], requires_grad=True)\n","tensor([[ 2.,  4.,  6.],\n","        [ 8., 10., 12.],\n","        [14., 16., 18.],\n","        [20., 22., 24.],\n","        [26., 28., 30.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dZ7PYiy1cj2x","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"y9-qPhkMb5YC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}