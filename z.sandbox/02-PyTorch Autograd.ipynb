{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02-PyTorch Autograd.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1GmlPyMQec6u","colab_type":"code","outputId":"a061fa09-8596-4612-cdd9-6a7fb2ee9606","executionInfo":{"status":"ok","timestamp":1577035058186,"user_tz":360,"elapsed":3135,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":395}},"source":["# https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95\n","import torch\n","\n","# Creating the graph\n","x = torch.tensor(1.0, requires_grad = True)\n","y = torch.tensor(2.0)\n","z = x * y\n","\n","# Displaying\n","for i, name in zip([x, y, z], \"xyz\"):\n","    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n","grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\n\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["x\n","data: 1.0\n","requires_grad: True\n","grad: None\n","grad_fn: None\n","is_leaf: True\n","\n","y\n","data: 2.0\n","requires_grad: False\n","grad: None\n","grad_fn: None\n","is_leaf: True\n","\n","z\n","data: 2.0\n","requires_grad: True\n","grad: None\n","grad_fn: <MulBackward0 object at 0x7f795b007e48>\n","is_leaf: False\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"srR-7rn6g4ni","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3hWioMKohIG-","colab_type":"text"},"source":["## The backward() function\n","\n","Backward is the function which actually calculates the gradient by passing its argument (`1x1` unit tensor by default) through the backward graph all the way up to every leaf node traceable from the calling root tensor. The calculated gradients are then stored in `.grad` of every leaf node. Remember, the backward graph is already made dynamically during the forward pass. Backward function only calculates the gradient using the already made graph and stores them in leaf nodes."]},{"cell_type":"code","metadata":{"id":"xoN6MkSYg4uE","colab_type":"code","outputId":"e0b21a69-7cf6-4fd5-fcfe-18f8b1aeef76","executionInfo":{"status":"ok","timestamp":1577035065787,"user_tz":360,"elapsed":362,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import torch\n","# Creating the graph\n","x = torch.tensor(1.0, requires_grad = True)\n","z = x ** 3\n","z.backward()        # Computes the gradient \n","print(x.grad.data)  # Prints '3' which is dz/dx "],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensor(3.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lShBW3BTh8jA","colab_type":"text"},"source":["An important thing to notice is that when `z.backward()` is called, a tensor is automatically passed as `z.backward(torch.tensor(1.0))`. The `torch.tensor(1.0)` is the external gradient provided to terminate the chain rule gradient multiplications. This external gradient is passed as the input to the `MulBackward` function to further calculate the gradient of `x`. \n","\n","The dimension of tensor passed into `.backward()` must be the same as the dimension of the tensor whose gradient is being calculated."]},{"cell_type":"code","metadata":{"id":"VBTavyeyiWix","colab_type":"code","outputId":"66ac28dc-954b-403d-cdaa-1741932a1d98","executionInfo":{"status":"ok","timestamp":1576287078561,"user_tz":360,"elapsed":5461,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import torch\n","\n","x = torch.tensor([1.0, 2.0], requires_grad=True)\n","y = x**2\n","\n","y.backward(torch.FloatTensor([1.0, 1.0])) # the tensor provide to .backward acts as a weight\n","x.grad"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2., 4.])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"s2VvgwE8smUu","colab_type":"text"},"source":["The line `y.backward(torch.FloatTensor([1.0, 1.0]))` calculates the gradient using as an argument a tensor of ones. This is exactly similar to what PyTorch does with scalar; by default assigns a scalar tensor of `1`, but we don't see it. It happens internally.\n","\n","For tensors of greater dimensions, we have to explicitely declare the argument. In this simple case, because want to calculate the derivative of `x^2`, we don't want the output to be affected by any sort of weight, so we just use a tensor of ones. "]},{"cell_type":"code","metadata":{"id":"4s9r1gWBkAc-","colab_type":"code","outputId":"55cc50cf-96eb-4ba5-8953-cbbfff357e24","executionInfo":{"status":"ok","timestamp":1576287078563,"user_tz":360,"elapsed":5426,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# another way to put it would be\n","import torch\n","\n","x = torch.tensor([1.0, 2.0], requires_grad=True)\n","y = x**2\n","\n","#\n","w = torch.FloatTensor([1.0, 1.0])\n","y.backward(w)             # the tensor provide to .backward acts as a weight\n","x.grad                    # notice that we do not provide backward(x)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2., 4.])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"iF8UuxskklIp","colab_type":"code","outputId":"eb671be0-e064-4476-9306-d59e38cae748","executionInfo":{"status":"ok","timestamp":1576287078729,"user_tz":360,"elapsed":5556,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# This will happen if we provide x as argument of .backward()\n","import torch\n","\n","x = torch.tensor([1.0, 2.0], requires_grad=True)\n","y = x**2\n","\n","w = torch.FloatTensor([1.0, 1.0])\n","\n","y.backward(x)             # the tensor provide to .backward acts as the weights\n","x.grad                    \n","# out: tensor([2., 8.])   # x as an argument outputs the wrong derivatives"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2., 8.])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"M5pCWu5ClV7m","colab_type":"text"},"source":["For this simple example of calculating the derivative of `X^2`, `x` as an argument, outputs the wrong derivatives dy/dx because `[1.0, 2.0]` is not a tensor of ones."]},{"cell_type":"markdown","metadata":{"id":"4gGdE_VAlxzd","colab_type":"text"},"source":["## Increase the size of x\n","We make this more interesting and add a new variable `z`."]},{"cell_type":"code","metadata":{"id":"IiTT0FZdl13v","colab_type":"code","outputId":"03d2d21b-784a-4974-c153-8e4e654d62f3","executionInfo":{"status":"ok","timestamp":1576287078732,"user_tz":360,"elapsed":5523,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["import torch\n","\n","x = torch.tensor([1.0, 2.0], requires_grad=True)\n","y = x**2\n","z = y**3\n","\n","w = torch.FloatTensor([1.0, 1.0])\n","\n","#y.backward(w, retain_graph=True)             # the tensor provide to .backward acts as the weights\n","#print(x.grad)\n","\n","z.backward(w, retain_graph=True)\n","print(y.grad)\n","print(x.grad)\n","\n","z.backward(w, retain_graph=True)\n","print(y.grad)\n","print(x.grad)\n","\n","# why y is None?"],"execution_count":0,"outputs":[{"output_type":"stream","text":["None\n","tensor([  6., 192.])\n","None\n","tensor([ 12., 384.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MS_UxME2sY7d","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yUHpMjhErmrw","colab_type":"text"},"source":["In the previous operation, there is only one independent variable, and that is `x`. Both `y` and `z` depend on `x`, and `z` depend on `y`. When we apply the derivative on z respect of y we get `None` because is not an independent variable. Internally, the gradient function has the vale of **None**, so, if nothing changes respect of `y`, then it returns its original value in PyTorch.\n","\n","Let's see the next example:\n","\n"]},{"cell_type":"code","metadata":{"id":"ZQr8hR11sd1Z","colab_type":"code","outputId":"c6daec6b-0175-44a8-c19a-06759cbdf258","executionInfo":{"status":"ok","timestamp":1576287078739,"user_tz":360,"elapsed":5487,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# https://www.javatpoint.com/gradient-with-pytorch\n","import torch\n","\n","x = torch.tensor([2.0, 2.0], requires_grad=True) # independent variable\n","z = torch.tensor([4.0, 4.0], requires_grad=True) # independent variable\n","\n","y = x**2 + z**3  \n","# Its partial derivatives are:\n","#    dy/dx = 2x;     \n","#    dy/dz = 3z^2; \n","\n","w = torch.FloatTensor([1.0, 1.0])  # weights\n","\n","y.backward(w, retain_graph=True)\n","print(x.grad)\n","print(z.grad)\n","\n","y.backward(w, retain_graph=True)\n","print(x.grad)\n","print(z.grad)\n","\n","y.backward(w, retain_graph=True)\n","print(x.grad)\n","print(z.grad)\n","\n","# at every pass, the derivative is accumulated\n","# there is no second derivative calculated"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([4., 4.])\n","tensor([48., 48.])\n","tensor([8., 8.])\n","tensor([96., 96.])\n","tensor([12., 12.])\n","tensor([144., 144.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oFLmJFEEtGdS","colab_type":"text"},"source":["Now we get the right derivative computations, because `y` is the only dependent variable of `x` and `z`.\n"]},{"cell_type":"markdown","metadata":{"id":"5mGfuwc6zZGr","colab_type":"text"},"source":["## What happens if turn off `retain_graph`\n","We will get an error on the second and consecutive passes.\n","\n","     RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.`"]},{"cell_type":"code","metadata":{"id":"P-Lm9oIfloyU","colab_type":"code","outputId":"5a2b1f49-d44e-4364-f48d-55ae999a680a","executionInfo":{"status":"ok","timestamp":1576287657045,"user_tz":360,"elapsed":375,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# https://www.javatpoint.com/gradient-with-pytorch\n","import torch\n","\n","x = torch.tensor([2.0, 2.0], requires_grad=True) # independent variable\n","z = torch.tensor([4.0, 4.0], requires_grad=True) # independent variable\n","\n","y = x**2 + z**3  \n","# dy/dx = 2x;     \n","# dy/dz = 3z^2; \n","\n","\n","w = torch.FloatTensor([1.0, 1.0])  # weights\n","\n","y.backward(w)\n","print(x.grad)\n","print(z.grad)\n","\n","try: \n","   y.backward(w)\n","   print(x.grad)\n","   print(z.grad)\n","except RuntimeError:\n","   print(\"Trying backward a second time when buffers are empty\")\n","\n","try:\n","    y.backward(w)\n","except RuntimeError:\n","    print(\"Trying backward a second time when buffers are empty\")\n","else: \n","    print(x.grad)\n","    print(z.grad)\n","\n","# at every pass, the derivative is accumulated\n","# there is no second derivative calculated"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([4., 4.])\n","tensor([48., 48.])\n","Trying backward a second time when buffers are empty\n","Trying backward a second time when buffers are empty\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Y97i7rcAEQI2","colab_type":"text"},"source":["## calculating gradients per element emptying the grad accumulator\n","For neural networks, we usually use **loss** to assess how well the network has learned to classify the input image (or other tasks). The loss term is usually a scalar value.\n","\n","The gradient arguments of the backward() method is used to calculate a weighted sum of each element of a Variable w.r.t the leaf Variable. These weights are just the derivative of final loss w.r.t each element of the intermediate variable.\n"]},{"cell_type":"code","metadata":{"id":"VpZmPOzlERqe","colab_type":"code","outputId":"20661c6e-7be1-44a4-9909-a483dd3e6576","executionInfo":{"status":"ok","timestamp":1576288020520,"user_tz":360,"elapsed":409,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["# https://stackoverflow.com/a/47026836/5270873\n","from torch.autograd import Variable\n","import torch\n","\n","x = Variable(torch.FloatTensor([[1, 2, 3, 4]]), requires_grad=True)\n","z = 2*x\n","loss = z.sum(dim=1) # scalar\n","\n","\n","# do backward for first element of z\n","z.backward(torch.FloatTensor([[1, 0, 0, 0]]), retain_graph=True)\n","print(x.grad.data)\n","x.grad.data.zero_() #remove gradient in x.grad, or it will be accumulated\n","\n","\n","\n","# do backward for second element of z\n","z.backward(torch.FloatTensor([[0, 1, 0, 0]]), retain_graph=True)\n","print(x.grad.data)\n","x.grad.data.zero_()\n","\n","\n","# do backward for all elements of z, with weight equal to the derivative of\n","# loss w.r.t z_1, z_2, z_3 and z_4\n","z.backward(torch.FloatTensor([[1, 1, 1, 1]]), retain_graph=True)\n","print(x.grad.data)\n","x.grad.data.zero_()\n","\n","\n","\n","# or we can directly backprop using loss\n","loss.backward() # equivalent to loss.backward(torch.FloatTensor([1.0]))\n","print(x.grad.data)    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[2., 0., 0., 0.]])\n","tensor([[0., 2., 0., 0.]])\n","tensor([[2., 2., 2., 2.]])\n","tensor([[2., 2., 2., 2.]])\n"],"name":"stdout"}]}]}