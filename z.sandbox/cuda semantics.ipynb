{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cuda semantics.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"d8a90gwYk9Gj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"614154d9-7ef7-4d96-f5e9-877f857c49d5","executionInfo":{"status":"error","timestamp":1576296551214,"user_tz":360,"elapsed":851,"user":{"displayName":"ALFONSO REYES","photoUrl":"","userId":"07474635334369434839"}}},"source":["import torch\n","\n","\n","cuda = torch.device('cuda')     # Default CUDA device\n","ncuda = torch.cuda.device_count()\n","cuda0 = torch.device('cuda:0')\n","\n","\n","cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n","\n","x = torch.tensor([1., 2.], device=cuda0)\n","# x.device is device(type='cuda', index=0)\n","y = torch.tensor([1., 2.]).cuda()\n","# y.device is device(type='cuda', index=0)\n","\n","with torch.cuda.device(0):\n","    # allocates a tensor on GPU 1\n","    a = torch.tensor([1., 2.], device=cuda)\n","\n","    # transfers a tensor from CPU to GPU 1\n","    b = torch.tensor([1., 2.]).cuda()\n","    # a.device and b.device are device(type='cuda', index=1)\n","\n","    # You can also use ``Tensor.to`` to transfer a tensor:\n","    b2 = torch.tensor([1., 2.]).to(device=cuda)\n","    # b.device and b2.device are device(type='cuda', index=1)\n","\n","    c = a + b\n","    # c.device is device(type='cuda', index=1)\n","\n","    z = x + y\n","    # z.device is device(type='cuda', index=0)\n","\n","    # even within a context, you can specify the device\n","    # (or give a GPU index to the .cuda call)\n","    d = torch.randn(2, device=cuda2)\n","    e = torch.randn(2).to(cuda2)\n","    f = torch.randn(2).cuda(cuda2)\n","    # d.device, e.device, and f.device are all device(type='cuda', index=2)"],"execution_count":6,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-582d69d241fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# even within a context, you can specify the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# (or give a GPU index to the .cuda call)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcuda2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcuda2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal"]}]}]}